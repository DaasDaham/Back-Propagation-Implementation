{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackPropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaasDaham/Back-Propagation-Implementation/blob/main/BackPropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMga8SBhg26v",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "b4e92798-cb0c-4cf0-fe17-ee7a775ca719"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e3212ce-0952-4bf2-bdcd-37c033cee41b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7e3212ce-0952-4bf2-bdcd-37c033cee41b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving val_set.pkl to val_set.pkl\n",
            "Saving train_set.pkl to train_set.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4bpLKVV-xvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5811cc-9c2f-455f-acda-8463a662f610"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from sklearn import datasets\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import PIL\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, multilabel_confusion_matrix\n",
        "!matplotlib inline\n",
        "\n",
        "\n",
        "class MLPClassifier:\n",
        "    \"\"\"\n",
        "    My implementation of a Neural Network Classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    acti_fns = [\"relu\", \"sigmoid\", \"linear\", \"tanh\", \"softmax\"]\n",
        "    avl_optims = [\"adagrad\", \"gd\", \"gdm\", \"adam\", \"rmsprop\",\"nestov\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers,\n",
        "        layer_sizes,\n",
        "        activation,\n",
        "        learning_rate,\n",
        "        batch_size,\n",
        "        optimizer,\n",
        "        num_epochs,\n",
        "        scaling_factor=0.01,\n",
        "        regularization=\"None\",\n",
        "        reg_param=0,\n",
        "        initialization = 'random'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializing a new MyNeuralNetwork object\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers : int value specifying the number of layers\n",
        "\n",
        "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
        "\n",
        "        activation : string specifying the activation function to be used\n",
        "                                 possible inputs: relu, sigmoid, linear, tanh\n",
        "\n",
        "        learning_rate : float value specifying the learning rate to be used\n",
        "\n",
        "        batch_size : int value specifying the batch size to be used\n",
        "\n",
        "        num_epochs : int value specifying the number of epochs to be used\n",
        "        \"\"\"\n",
        "\n",
        "        if activation not in self.acti_fns:\n",
        "            raise Exception(\"Incorrect Activation Function\")\n",
        "        if optimizer not in self.avl_optims:\n",
        "            raise Exception(\"Incorrect Optimizer\")\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.n_layers = n_layers\n",
        "        self.scaling_factor = scaling_factor\n",
        "\n",
        "        # Initialize Weights\n",
        "        self.weight_draw = self.random_init\n",
        "\n",
        "        # Initialize activation function\n",
        "        if activation == \"sigmoid\":\n",
        "            self.afn = self.sigmoid\n",
        "            self.d_afn = self.sigmoid_grad\n",
        "        elif activation == \"relu\":\n",
        "            self.afn = self.relu\n",
        "            self.d_afn = self.relu_grad\n",
        "        elif activation == \"tanh\":\n",
        "            self.afn = self.tanh\n",
        "            self.d_afn = self.tanh_grad\n",
        "\n",
        "        self.regtype=\"None\"\n",
        "        if regularization == \"l1\":\n",
        "            self.regtype = \"l1\"\n",
        "            self.reg_lambda = reg_param\n",
        "        if regularization == \"l2\":\n",
        "            self.regtype = \"l2\"\n",
        "            self.reg_lambda = reg_param\n",
        "        if regularization == \"dropout\":\n",
        "            self.keep_probs = reg_param\n",
        "        # [784, 256, 128, 64, 10]\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.final_hidden_layer_tsne = []\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.num_epochs = num_epochs\n",
        "        temp_layers = layer_sizes\n",
        "        if initialization == 'xavier':\n",
        "          temp_layers.insert(0, temp_layers[0])\n",
        "          self.weights = np.array([\n",
        "              np.random.normal(0, 1/math.sqrt(temp_layers[dims-1]), size = (temp_layers[dims+1], temp_layers[dims]))\n",
        "              for dims in range(1, len(temp_layers)-1)\n",
        "          ])\n",
        "          temp_layers.pop(0)\n",
        "        elif initialization == 'he':\n",
        "          temp_layers.insert(0, temp_layers[0])\n",
        "          self.weights = np.array([\n",
        "              np.random.normal(0, math.sqrt(2/temp_layers[dims-1]), size = (temp_layers[dims+1], temp_layers[dims]))\n",
        "              for dims in range(1, len(temp_layers)-1)\n",
        "          ])\n",
        "          temp_layers.pop(0)\n",
        "        else:\n",
        "          self.weights = np.array([\n",
        "              self.weight_draw((layer_sizes[dims], layer_sizes[dims + 1]))\n",
        "              for dims in range(len(layer_sizes) - 1)\n",
        "          ])\n",
        "        self.biases = np.array([np.zeros(shape=(dim, 1)) for dim in layer_sizes[1:]])\n",
        "        self.usingadam = False\n",
        "        if optimizer == \"adagrad\":\n",
        "            self.optim = self.update_params_ada_grad\n",
        "            self.Gwt = np.array([np.zeros_like(wt) for wt in self.weights])\n",
        "            self.Gbt = np.array([np.zeros_like(bt) for bt in self.biases])\n",
        "        elif optimizer == \"gd\":\n",
        "            self.optim = self.update_params\n",
        "        elif optimizer == \"gdm\":\n",
        "            self.optim=self.update_params_grad_momentum\n",
        "            self.vdw=np.array([np.zeros_like(wt) for wt in self.weights])\n",
        "            self.vdb=np.array([np.zeros_like(bt) for bt in self.biases])\n",
        "            self.beta=0.9\n",
        "        elif optimizer == \"nestov\":\n",
        "            self.optim = self.update_params_nestov_accelerated\n",
        "            self.beta = 0.9\n",
        "            self.vdw = np.array([np.zeros_like(wt) for wt in self.weights])\n",
        "            self.vdb = np.array([np.zeros_like(bt) for bt in self.biases])\n",
        "        elif optimizer == \"rmsprop\":\n",
        "            self.optim=self.update_params_rms_prop\n",
        "            self.sdw=np.array([np.zeros_like(wt) for wt in self.weights])\n",
        "            self.sdb=np.array([np.zeros_like(bt) for bt in self.biases])\n",
        "            self.beta=0.9\n",
        "            self.epsilon=1.0e-8\n",
        "        elif optimizer == \"adam\":\n",
        "            self.usingadam = True\n",
        "            self.optim= self.update_params_adam\n",
        "            self.sdw=np.array([np.zeros_like(wt) for wt in self.weights])\n",
        "            self.sdb=np.array([np.zeros_like(bt) for bt in self.biases])\n",
        "            self.vdw=np.array([np.zeros_like(wt) for wt in self.weights])\n",
        "            self.vdb=np.array([np.zeros_like(bt) for bt in self.biases])\n",
        "            self.beta1=0.9\n",
        "            self.beta2=0.999\n",
        "            self.epsilon=1.0e-8\n",
        "\n",
        "\n",
        "    def relu(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        Z = np.copy(X)\n",
        "        Z[Z < 0] = Z[Z < 0] * 0.01\n",
        "        return Z\n",
        "\n",
        "    def relu_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        dx = np.ones_like(X)\n",
        "        dx[X < 0] = 0.01\n",
        "        return dx\n",
        "\n",
        "    def sigmoid(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "    def sigmoid_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        tmp = 1 / (1 + np.exp(-X))\n",
        "        return tmp * (1 - tmp)\n",
        "\n",
        "    def tanh(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        return (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
        "\n",
        "    def tanh_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        tmp = (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
        "        return 1 - tmp ** 2\n",
        "\n",
        "    def softmax(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        recenteredX = X - np.max(X)\n",
        "        exps = np.exp(recenteredX)\n",
        "        return exps / np.sum(exps)\n",
        "\n",
        "    def batch_softmax(self, X):\n",
        "        return np.array([self.softmax(row) for row in X])\n",
        "\n",
        "    def softmax_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Softmax activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        s = self.softmax(X)\n",
        "        s = s.reshape(-1,1)\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    def batch_softmax_grad(self, X):\n",
        "        s = self.batch_softmax(X)\n",
        "        return np.array([self.softmax_grad(row) for row in s])\n",
        "\n",
        "    def random_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Random Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        return np.random.randn(shape[1], shape[0]) * self.scaling_factor\n",
        "\n",
        "    def loss_fn(self, X, y):\n",
        "        \"\"\"\n",
        "        X : training samples, shape (num_examples x num_classes)\n",
        "        y : labels, shape = (num_examples x 1)\n",
        "        returns multi class cross entropy loss\n",
        "        \"\"\"\n",
        "        N = X.shape[0]\n",
        "        ce = (-np.sum(y * np.log(X))) / N\n",
        "        return ce\n",
        "\n",
        "    def d_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        X: predictions from last layer after activating with either sigmoid or softmax\n",
        "        y: corresponding outputs\n",
        "        Returns: gradient for CE loss only when input is passed through sigmoid or softmax\n",
        "        \"\"\"\n",
        "        return X - y\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        X: Input in shape: samples * features\n",
        "        Returns: Outputs after forward prop\n",
        "        \"\"\"\n",
        "        outputs = []\n",
        "        for x in X:\n",
        "            a = x.reshape(-1, 1)\n",
        "            last_layer_check = 0\n",
        "            last_layer_index = len(self.weights) - 1\n",
        "            for wl, bl in zip(self.weights, self.biases):\n",
        "                a = np.dot(wl, a) + bl\n",
        "                if last_layer_check == last_layer_index:\n",
        "                    a = self.softmax(a)\n",
        "                else:\n",
        "                    a = self.afn(a)\n",
        "                last_layer_check += 1\n",
        "            outputs.append(a.reshape(-1))\n",
        "        return np.array(outputs)\n",
        "\n",
        "    def back(self, X, y, batch_loss, fin_hidden_layer):\n",
        "        \"\"\"\n",
        "        Back propogate and calculate Delta W and Delta B for a given mini-batch\n",
        "        batch_loss: to calculate and store loss for the complete batch\n",
        "        Implements Backprop algorithm\n",
        "        \"\"\"\n",
        "        deltaW = np.copy(self.weights)\n",
        "        deltaB = np.copy(self.biases)\n",
        "        batch_size = X.shape[0]\n",
        "        a = X\n",
        "        pre_acts = [a]\n",
        "        post_acts = [a]\n",
        "        last_layer_check = 0\n",
        "        last_layer_index = len(self.weights) - 1\n",
        "        drop_prob_idx = 0\n",
        "        for wl, bl in zip(self.weights, self.biases):\n",
        "            bias_l_cp = np.copy(bl).reshape(-1)\n",
        "            bias_l_cp = np.array(\n",
        "                [\n",
        "                    bias_l_cp,\n",
        "                ]\n",
        "                * batch_size\n",
        "            )\n",
        "            a = a.dot(wl.T) + bias_l_cp\n",
        "            pre_acts.append(a)\n",
        "            if last_layer_check == last_layer_index:\n",
        "                a = self.batch_softmax(a)\n",
        "            else:\n",
        "                a = self.afn(a)\n",
        "                if self.regtype==\"dropout\":\n",
        "                    curr_mask = (np.random.rand(a.shape[0], a.shape[1]) < self.keep_probs[drop_prob_idx])/self.keep_probs[drop_prob_idx]\n",
        "                    a = np.multiply(a, curr_mask)\n",
        "                    drop_prob_idx += 1\n",
        "            post_acts.append(a)\n",
        "            last_layer_check += 1\n",
        "        loss = self.loss_fn(post_acts[-1], y)\n",
        "        batch_loss.append(loss)\n",
        "        last_layer_loss = self.d_loss(post_acts[-1], y).reshape(y.shape[0], 1, y.shape[1])\n",
        "        last_layer_loss_gd = np.matmul(last_layer_loss, self.batch_softmax_grad(pre_acts[-1]))\n",
        "        last_layer_loss_gd = np.squeeze(last_layer_loss_gd)\n",
        "        deltaW[-1] = (last_layer_loss_gd.T).dot(post_acts[-2])\n",
        "        deltaB[-1] = np.sum(last_layer_loss_gd, axis=0).reshape(-1, 1)\n",
        "        curr_layer_gd = last_layer_loss_gd\n",
        "        for curr_layer_id in range(len(self.layer_sizes) - 2, 0, -1):\n",
        "            curr_layer_gd = curr_layer_gd.dot(self.weights[curr_layer_id]) * self.d_afn(\n",
        "                pre_acts[curr_layer_id]\n",
        "            )\n",
        "            deltaW[curr_layer_id - 1] = (curr_layer_gd.T).dot(\n",
        "                post_acts[curr_layer_id - 1]\n",
        "            )\n",
        "            deltaB[curr_layer_id - 1] = np.sum(curr_layer_gd, axis=0).reshape(-1, 1)\n",
        "        return deltaW, deltaB\n",
        "\n",
        "    def combine(self, X, y):\n",
        "        \"\"\"\n",
        "        Function: To combine two Arrays X and y along column\n",
        "        \"\"\"\n",
        "        y_shape = y.shape[1]\n",
        "        data = np.concatenate((X, y), axis=1)\n",
        "        return data\n",
        "\n",
        "    def get_x_y(self, data, y_shape):\n",
        "        \"\"\"\n",
        "        Performs the opposite function of combine function\n",
        "        Splits X and y from combined data\n",
        "        Arguments:\n",
        "        data = Combined data to be splitted\n",
        "        y_shape = number of columns required to be splitted\n",
        "        \"\"\"\n",
        "        y = data[:, -y_shape : data.shape[1]]\n",
        "        x = data[:, :-y_shape]\n",
        "        return x, y        \n",
        "        \n",
        "\n",
        "    def update_params_grad_momentum(self, X_train_batch, y_train_batch, lr):\n",
        "        \"\"\"\n",
        "        Update weights and Biases using backprop\n",
        "        lr: learning rate\n",
        "        returns: average batch loss and final hidden layer ouput used for TSNE plot\n",
        "        \"\"\"\n",
        "        batch_loss = []\n",
        "        fin_hidden_layer = []\n",
        "        delta_W, delta_b = self.back(\n",
        "            X_train_batch, y_train_batch, batch_loss, fin_hidden_layer\n",
        "        )\n",
        "        \"\"\"Have to be initialzed from start\n",
        "        self.optim=self.update_params_grad_momentum\n",
        "        self.vdw=np.zeros_like(delta_W)\n",
        "        self.vdb=np.zeros_like(delta_b)\n",
        "        self.beta=0.9\n",
        "\n",
        "        \"\"\"\n",
        "        if self.regtype == \"l1\":\n",
        "            delta_W = [dW+(np.sign(W)*self.reg_lambda) for dW, W in zip(delta_W, self.weights)]\n",
        "        if self.regtype == \"l2\":\n",
        "            delta_W = [dW+((2*self.reg_lambda)*W) for dW, W in zip(delta_W, self.weights)]\n",
        "\n",
        "        self.vdw= [self.beta*vd+(1-self.beta)*dw for vd,dw in zip(self.vdw,delta_W)]\n",
        "        self.vdb= [self.beta*vd+(1-self.beta)*db for vd,db in zip(self.vdb,delta_b)]\n",
        "\n",
        "        self.weights = [W - (lr* dW) for W,dW in zip(self.weights, self.vdw)]\n",
        "        self.biases = [b - (lr* db) for b,db in zip(self.biases, self.vdb)]\n",
        "        return np.average(batch_loss), fin_hidden_layer\n",
        "      \n",
        "    def update_params_rms_prop(self, X_train_batch, y_train_batch, lr):\n",
        "        \"\"\"\n",
        "        Update weights and Biases using backprop\n",
        "        lr: learning rate\n",
        "        returns: average batch loss and final hidden layer ouput used for TSNE plot\n",
        "        \"\"\"\n",
        "        batch_loss = []\n",
        "        fin_hidden_layer = []\n",
        "        delta_W, delta_b = self.back(\n",
        "            X_train_batch, y_train_batch, batch_loss, fin_hidden_layer\n",
        "        )\n",
        "        \n",
        "        \"\"\"Will be used in initialization\n",
        "        self.optim=self.update_params_rms_prop\n",
        "        self.sdw=np.zeros_like(self.weights)\n",
        "        self.sdb=np.zeros_like(self.biases)\n",
        "        self.beta=0.9\n",
        "        self.epsilon=1.0e-8\n",
        "        \"\"\"\n",
        "\n",
        "        dw_sq= np.square(delta_W)\n",
        "        db_sq= np.square(delta_b)\n",
        "\n",
        "        self.sdw= [self.beta*sd+(1-self.beta)*dwsq for sd,dw,dwsq in zip(self.sdw,delta_W,dw_sq)]\n",
        "        self.sdb= [self.beta*sd+(1-self.beta)*dbsq for sd,db,dbsq in zip(self.sdb,delta_b,db_sq)]\n",
        "\n",
        "        self.weights = [W -((lr* dW)/(np.sqrt(dws)+self.epsilon)) for W,dws,dW in zip(self.weights, self.sdw,delta_W)]\n",
        "        self.biases = [b - ((lr* db)/(np.sqrt(dbs)+self.epsilon)) for b,dbs,db in zip(self.biases, self.sdb,delta_b)]\n",
        "        return np.average(batch_loss), fin_hidden_layer\n",
        "\n",
        "    def update_params_adam(self, X_train_batch, y_train_batch, lr, epoch):\n",
        "        \"\"\"\n",
        "        Update weights and Biases using backprop\n",
        "        lr: learning rate\n",
        "        returns: average batch loss and final hidden layer ouput used for TSNE plot\n",
        "        \"\"\"\n",
        "        batch_loss = []\n",
        "        fin_hidden_layer = []\n",
        "        delta_W, delta_b = self.back(\n",
        "            X_train_batch, y_train_batch, batch_loss, fin_hidden_layer\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "        Will be initiliazed at the init\n",
        "        sdw=np.zeros_like(delta_W)\n",
        "        sdb=np.zeros_like(delta_b)\n",
        "        vdw=np.zeros_like(delta_W)\n",
        "        vdb=np.zeros_like(delta_b)\n",
        "        \n",
        "        beta1=0.9\n",
        "        beta2=0.999\n",
        "        epsilon=1.0e-8\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        dw_sq= np.square(delta_W)\n",
        "        db_sq= np.square(delta_b)\n",
        "\n",
        "        self.vdw= [(self.beta1*vd)+((1-self.beta1)*dw) for vd,dw in zip(self.vdw,delta_W)]\n",
        "        self.vdb= [(self.beta1*vd)+((1-self.beta1)*db) for vd,db in zip(self.vdb,delta_b)]\n",
        "\n",
        "        self.sdw= [(self.beta2*sd)+((1-self.beta2)*dwsq) for sd,dw,dwsq in zip(self.sdw,delta_W,dw_sq)]\n",
        "        self.sdb= [(self.beta2*sd)+((1-self.beta2)*dbsq) for sd,db,dbsq in zip(self.sdb,delta_b,db_sq)]\n",
        "\n",
        "        first_den = (1-(self.beta1**epoch))\n",
        "        second_den = (1-(self.beta2**epoch))\n",
        "\n",
        "        # self.vdw= [vd/first_den for vd in self.vdw]\n",
        "        # self.vdb= [vd/first_den for vd in self.vdb]\n",
        "\n",
        "        # self.sdw= [sd/second_den for sd in self.sdw]\n",
        "        # self.sdb= [sd/second_den for sd in self.sdb]\n",
        "\n",
        "        self.weights = [W -((lr* dwv)/(np.sqrt(dws)+self.epsilon)) for W,dws,dwv in zip(self.weights, self.sdw,self.vdw)]\n",
        "        self.biases = [b - ((lr* dbv)/(np.sqrt(dbs)+self.epsilon)) for b,dbs,dbv in zip(self.biases, self.sdb,self.vdb)]\n",
        "        return np.average(batch_loss), fin_hidden_layer\n",
        "\n",
        "    def update_params_nestov_accelerated(self, X_train_batch, y_train_batch, lr, epsilon=1e-8):\n",
        "        \"\"\"\n",
        "        Update weights and Biases using backprop\n",
        "        lr: learning rate\n",
        "        returns: average batch loss and final hidden layer ouput used for TSNE plot\n",
        "        \"\"\"\n",
        "        batch_loss = []\n",
        "        fin_hidden_layer = []\n",
        "\n",
        "        self.weights = np.array([wt - self.beta*vw for wt, vw in zip(self.weights, self.vdw)])\n",
        "        self.biases = np.array([bt - self.beta*vb for bt, vb in zip(self.biases, self.vdb)])\n",
        "\n",
        "        delta_W, delta_b = self.back(\n",
        "            X_train_batch, y_train_batch, batch_loss, fin_hidden_layer\n",
        "        )\n",
        "\n",
        "        #backpropgation\n",
        "        vdw = np.array([self.beta*vw + lr*dw for vw, dw in zip(self.vdw, delta_W)])\n",
        "        vdb = np.array([self.beta*vb + lr*db for vb, db in zip(self.vdb, delta_b)])\n",
        "\n",
        "        self.weights = np.array([wt - self.beta*vw - vw for wt, vw in zip(self.weights, self.vdw)])\n",
        "        self.biases = np.array([bt - self.beta*vb -vb for bt, vb in zip(self.biases, self.vdb)])\n",
        "\n",
        "        # self.weights = self.weights + self.beta*self.vdw-vdw\n",
        "        # self.biases = self.biases + self.beta*self.vdb - vdb\n",
        "        self.vdw = vdw \n",
        "        self.vdb = vdb\n",
        "        ###\n",
        "        return np.average(batch_loss), fin_hidden_layer\n",
        "\n",
        "    def update_params_ada_grad(self, X_train_batch, y_train_batch, lr, epsilon=1e-8):\n",
        "        \"\"\"\n",
        "        Update weights and Biases using backprop\n",
        "        lr: learning rate\n",
        "        returns: average batch loss and final hidden layer ouput used for TSNE plot\n",
        "        \"\"\"\n",
        "        batch_loss = []\n",
        "        fin_hidden_layer = []\n",
        "        delta_W, delta_b = self.back(\n",
        "            X_train_batch, y_train_batch, batch_loss, fin_hidden_layer\n",
        "        )\n",
        "        for i in range(len(delta_W)):\n",
        "            self.Gwt[i] = self.Gwt[i]+delta_W[i]**2\n",
        "        for i in range(len(delta_b)):\n",
        "            self.Gbt[i] = self.Gbt[i]+delta_b[i]**2\n",
        "        \n",
        "        temp_gwt = [lr/np.sqrt(G+epsilon) for G in self.Gwt]\n",
        "        temp_gbt = [lr/np.sqrt(B+epsilon) for B in self.Gbt]\n",
        "\n",
        "        self.weights = [W - (Gii * dW) for W, Gii, dW in zip(self.weights, temp_gwt, delta_W)]\n",
        "        self.biases = [b - (Gii * db) for b, Gii, db in zip(self.biases, temp_gbt, delta_b)]\n",
        "        return np.average(batch_loss), fin_hidden_layer\n",
        "\n",
        "    def update_params(self, X_train_batch, y_train_batch, lr):\n",
        "        \"\"\"\n",
        "        Update weights and Biases using backprop\n",
        "        lr: learning rate\n",
        "        returns: average batch loss and final hidden layer ouput used for TSNE plot\n",
        "        \"\"\"\n",
        "        batch_loss = []\n",
        "        fin_hidden_layer = []\n",
        "        delta_W, delta_b = self.back(\n",
        "            X_train_batch, y_train_batch, batch_loss, fin_hidden_layer\n",
        "        )\n",
        "        self.weights = [\n",
        "            W - (lr * dW / len(X_train_batch)) for W, dW in zip(self.weights, delta_W)\n",
        "        ]\n",
        "        self.biases = [\n",
        "            b - (lr * db / len(X_train_batch)) for b, db in zip(self.biases, delta_b)\n",
        "        ]\n",
        "        return np.average(batch_loss), fin_hidden_layer\n",
        "\n",
        "    def fit(self, X, y, X_val=[], y_val=[]):\n",
        "        \"\"\"\n",
        "        Fitting (training) the linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : an instance of self\n",
        "        \"\"\"\n",
        "        y_shape = y.shape[1]\n",
        "        for epoch in range(self.num_epochs):\n",
        "            data = self.combine(X, y)\n",
        "            np.random.shuffle(data)\n",
        "            folds = np.array_split(data, data.shape[0] / self.batch_size)\n",
        "            temp_loss = []\n",
        "            for i in range(len(folds)):\n",
        "                X_train_batch, y_train_batch = self.get_x_y(folds[i], y_shape)\n",
        "                if self.usingadam == True:\n",
        "                    batch_loss, fin_hidden_layer = self.update_params_adam(\n",
        "                        X_train_batch, y_train_batch, self.lr, (epoch+1)\n",
        "                    )\n",
        "                else:\n",
        "                    batch_loss, fin_hidden_layer = self.optim(\n",
        "                        X_train_batch, y_train_batch, self.lr\n",
        "                    )\n",
        "                temp_loss.append(batch_loss)\n",
        "            self.training_loss.append(np.average(temp_loss))\n",
        "            val_error = self.get_error(X_val, y_val)\n",
        "            self.validation_loss.append(val_error)\n",
        "            val_accuracy = self.score(X_val, y_val)\n",
        "            print(\n",
        "                \"Epoch {0}: Validation Accuracy: {1:.2f}, Training Loss: {2}, Validation Loss {3}\".format(\n",
        "                    epoch + 1, val_accuracy, self.training_loss[-1], val_error\n",
        "                )\n",
        "            )\n",
        "        return self\n",
        "\n",
        "    def get_error(self, X, y):\n",
        "        \"\"\"\n",
        "        Used to get CE error\n",
        "        \"\"\"\n",
        "        y_pred = self.forward(X)\n",
        "        error = self.loss_fn(y_pred, y)\n",
        "        return error\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predicting probabilities using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the\n",
        "                class wise prediction probabilities.\n",
        "        \"\"\"\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        y_pred_probs = self.forward(X)\n",
        "        return y_pred_probs\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "        \"\"\"\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        y_pred = self.forward(X)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "    def get_params(self):\n",
        "        return self.weights, self.biases\n",
        "     \n",
        "    def plot_roc(self, X_test, y_test, plot_name):\n",
        "        y_probs = self.predict_proba(X_test)\n",
        "        for i in range(10):\n",
        "            lr_fpr, lr_tpr, _ = roc_curve(y_test[:,i],y_probs[:,i])\n",
        "            plt.plot(lr_fpr, lr_tpr, marker='.', label='Number-'+str(i))\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.legend()\n",
        "        plt.savefig(plot_name+\".png\")\n",
        "\n",
        "    def plot_losses(self, hidden_layer_afn, optimizer_name, plot_name):\n",
        "        epochs = np.arange(self.num_epochs)\n",
        "        train_losses = self.training_loss\n",
        "        test_loss = self.validation_loss\n",
        "        fig, ax = plt.subplots(figsize=(12,6.5))\n",
        "        plt.plot(epochs, train_losses, label=\"Training loss\")\n",
        "        plt.plot(epochs, test_loss, label=\"Validation Loss\")\n",
        "        ax.legend()\n",
        "        plt.title(\"Train, Validation loss | {} | {}\".format(hidden_layer_afn, optimizer_name))\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('Training Loss, Validation Loss')\n",
        "        plt.savefig(plot_name+\".png\")\n",
        "        plt.close()\n",
        "\n",
        "    def plot_confusion_matrix(self, X_test, y_test, plot_name):\n",
        "        y_pred = self.predict(X_test)\n",
        "        cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "        print(cm)\n",
        "        cmd = ConfusionMatrixDisplay(cm, display_labels=['0','1','2','3','4','5','6','7','8','9'])\n",
        "        cmd.plot()\n",
        "        plt.savefig(plot_name+\".png\")\n",
        "        plt.close()\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        acc : float value specifying the accuracy of the model on the provided testing set\n",
        "        \"\"\"\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        pred = self.forward(X)\n",
        "        return np.sum(np.argmax(y, axis=1) == np.argmax(pred, axis=1)) / len(y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: matplotlib: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1S9Ec11oa9Q"
      },
      "source": [
        "train = np.load('train_set.pkl', allow_pickle=True)\n",
        "test = np.load('val_set.pkl', allow_pickle=True)\n",
        "X_train = np.array([(np.array(i).flatten())/255 for i in train.iloc[:,0]])\n",
        "enc = OneHotEncoder()\n",
        "y_train = enc.fit_transform(np.array(train.iloc[:,1]).reshape(-1,1)).toarray()\n",
        "X_val = X_train[9000:]\n",
        "y_val = y_train[9000:]\n",
        "X_train = X_train[:9000]\n",
        "y_train = y_train[:9000]\n",
        "X_test = np.array([(np.array(i).flatten())/255 for i in test.iloc[:,0]])\n",
        "enc = OneHotEncoder()\n",
        "y_test = enc.fit_transform(np.array(test.iloc[:,1]).reshape(-1,1)).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bujjD2riobAC",
        "outputId": "616fe10c-a5a6-422f-a163-700d69653734"
      },
      "source": [
        "# Instantiating MLPClassifier class\n",
        "mlp = MLPClassifier(5, [784, 256, 128, 64, 10], \"relu\", 0.01, 64,\"gdm\", 100, scaling_factor=0.1, regularization=\"dropout\", reg_param=[0.2,0.2,0.2], initialization=\"xavier\") \n",
        "mlp.fit(X_train, y_train, X_val, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:119: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:792: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, order=order, subok=subok, copy=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(256, 784) Weights\n",
            "(128, 256) Weights\n",
            "(64, 128) Weights\n",
            "(10, 64) Weights\n",
            "Epoch 1: Validation Accuracy: 0.43, Training Loss: 2.1630961368404535, Validation Loss 1.847174173170763\n",
            "Epoch 2: Validation Accuracy: 0.76, Training Loss: 0.8160707448466026, Validation Loss 0.6822289701923832\n",
            "Epoch 3: Validation Accuracy: 0.87, Training Loss: 0.43564624758848236, Validation Loss 0.4342180550120704\n",
            "Epoch 4: Validation Accuracy: 0.88, Training Loss: 0.35404877169787197, Validation Loss 0.3992990332285246\n",
            "Epoch 5: Validation Accuracy: 0.91, Training Loss: 0.2849591042580806, Validation Loss 0.3345091316522048\n",
            "Epoch 6: Validation Accuracy: 0.92, Training Loss: 0.24564567309346166, Validation Loss 0.26059634889778044\n",
            "Epoch 7: Validation Accuracy: 0.92, Training Loss: 0.20410739189371455, Validation Loss 0.27431245693663675\n",
            "Epoch 8: Validation Accuracy: 0.93, Training Loss: 0.17649717435966591, Validation Loss 0.25643409552301727\n",
            "Epoch 9: Validation Accuracy: 0.93, Training Loss: 0.1493881460602113, Validation Loss 0.24911555845627634\n",
            "Epoch 10: Validation Accuracy: 0.93, Training Loss: 0.12836103494963272, Validation Loss 0.24631215657781944\n",
            "Epoch 11: Validation Accuracy: 0.95, Training Loss: 0.11043026463413186, Validation Loss 0.20131298428563366\n",
            "Epoch 12: Validation Accuracy: 0.94, Training Loss: 0.08960677528155724, Validation Loss 0.2240094822473719\n",
            "Epoch 13: Validation Accuracy: 0.94, Training Loss: 0.08164469902610122, Validation Loss 0.21976686376815577\n",
            "Epoch 14: Validation Accuracy: 0.93, Training Loss: 0.06515585064287449, Validation Loss 0.27342663898236175\n",
            "Epoch 15: Validation Accuracy: 0.94, Training Loss: 0.05933860512364306, Validation Loss 0.23815419115926853\n",
            "Epoch 16: Validation Accuracy: 0.94, Training Loss: 0.05453646355270222, Validation Loss 0.24927838676193495\n",
            "Epoch 17: Validation Accuracy: 0.94, Training Loss: 0.04421203480377054, Validation Loss 0.24459950683498657\n",
            "Epoch 18: Validation Accuracy: 0.94, Training Loss: 0.029950946825358964, Validation Loss 0.2883875745217795\n",
            "Epoch 19: Validation Accuracy: 0.94, Training Loss: 0.03170612648480077, Validation Loss 0.2339134268089901\n",
            "Epoch 20: Validation Accuracy: 0.94, Training Loss: 0.024805370635526684, Validation Loss 0.27316611898683274\n",
            "Epoch 21: Validation Accuracy: 0.93, Training Loss: 0.026872110703491384, Validation Loss 0.32654292547326863\n",
            "Epoch 22: Validation Accuracy: 0.94, Training Loss: 0.017449481783157507, Validation Loss 0.2617489056003997\n",
            "Epoch 23: Validation Accuracy: 0.95, Training Loss: 0.01235363850571414, Validation Loss 0.2893698233256801\n",
            "Epoch 24: Validation Accuracy: 0.93, Training Loss: 0.0232309906246563, Validation Loss 0.36439369738330685\n",
            "Epoch 25: Validation Accuracy: 0.95, Training Loss: 0.02385629112461655, Validation Loss 0.26350597693335853\n",
            "Epoch 26: Validation Accuracy: 0.95, Training Loss: 0.01413562784414188, Validation Loss 0.3026991885923723\n",
            "Epoch 27: Validation Accuracy: 0.95, Training Loss: 0.008171803524777492, Validation Loss 0.32396545607055527\n",
            "Epoch 28: Validation Accuracy: 0.95, Training Loss: 0.008200190539995749, Validation Loss 0.29550346894958907\n",
            "Epoch 29: Validation Accuracy: 0.95, Training Loss: 0.009755902974768608, Validation Loss 0.3179660310765666\n",
            "Epoch 30: Validation Accuracy: 0.94, Training Loss: 0.0039257572975980835, Validation Loss 0.3730403029426985\n",
            "Epoch 31: Validation Accuracy: 0.95, Training Loss: 0.0035053542884615047, Validation Loss 0.3923242442489863\n",
            "Epoch 32: Validation Accuracy: 0.94, Training Loss: 0.013551619142236931, Validation Loss 0.36155277272858594\n",
            "Epoch 33: Validation Accuracy: 0.96, Training Loss: 0.014442559666427357, Validation Loss 0.29938846724900836\n",
            "Epoch 34: Validation Accuracy: 0.93, Training Loss: 0.006042153689713554, Validation Loss 0.4040484877336145\n",
            "Epoch 35: Validation Accuracy: 0.95, Training Loss: 0.003664518783748981, Validation Loss 0.35482019004127535\n",
            "Epoch 36: Validation Accuracy: 0.95, Training Loss: 0.0026356135597908606, Validation Loss 0.3673801120819255\n",
            "Epoch 37: Validation Accuracy: 0.95, Training Loss: 0.0007961060449653251, Validation Loss 0.3757024620494933\n",
            "Epoch 38: Validation Accuracy: 0.95, Training Loss: 0.00036446026029681355, Validation Loss 0.37242938902313766\n",
            "Epoch 39: Validation Accuracy: 0.95, Training Loss: 0.0004129689912554752, Validation Loss 0.4010072459120771\n",
            "Epoch 40: Validation Accuracy: 0.95, Training Loss: 0.0004144703958954329, Validation Loss 0.40666098273650575\n",
            "Epoch 41: Validation Accuracy: 0.95, Training Loss: 0.00029878836167207334, Validation Loss 0.4042525762372457\n",
            "Epoch 42: Validation Accuracy: 0.95, Training Loss: 0.00034225456552315815, Validation Loss 0.41334859335880497\n",
            "Epoch 43: Validation Accuracy: 0.95, Training Loss: 0.0003047617264304505, Validation Loss 0.4258056541941377\n",
            "Epoch 44: Validation Accuracy: 0.95, Training Loss: 0.00033924125923082355, Validation Loss 0.42026286305353444\n",
            "Epoch 45: Validation Accuracy: 0.95, Training Loss: 0.0003056695527714543, Validation Loss 0.42819166115863955\n",
            "Epoch 46: Validation Accuracy: 0.95, Training Loss: 0.0003204886122341395, Validation Loss 0.45166017733971286\n",
            "Epoch 47: Validation Accuracy: 0.95, Training Loss: 0.00028373840884278885, Validation Loss 0.4432737498023386\n",
            "Epoch 48: Validation Accuracy: 0.94, Training Loss: 0.000292766215006662, Validation Loss 0.4867262077755099\n",
            "Epoch 49: Validation Accuracy: 0.95, Training Loss: 0.0004588602845551501, Validation Loss 0.46510949314572836\n",
            "Epoch 50: Validation Accuracy: 0.95, Training Loss: 0.000302362123290712, Validation Loss 0.4776092377149802\n",
            "Epoch 51: Validation Accuracy: 0.95, Training Loss: 0.0002487271980842264, Validation Loss 0.48604396718190457\n",
            "Epoch 52: Validation Accuracy: 0.95, Training Loss: 0.00025996591203709534, Validation Loss 0.4888464047323153\n",
            "Epoch 53: Validation Accuracy: 0.95, Training Loss: 0.00023753913992708526, Validation Loss 0.4985704398785731\n",
            "Epoch 54: Validation Accuracy: 0.95, Training Loss: 0.00022182164577170245, Validation Loss 0.5064751427987748\n",
            "Epoch 55: Validation Accuracy: 0.95, Training Loss: 0.0002344706333226992, Validation Loss 0.5107425272846003\n",
            "Epoch 56: Validation Accuracy: 0.95, Training Loss: 0.00021920122419174276, Validation Loss 0.5093818138007014\n",
            "Epoch 57: Validation Accuracy: 0.95, Training Loss: 0.00021384281492444677, Validation Loss 0.5229410690091577\n",
            "Epoch 58: Validation Accuracy: 0.95, Training Loss: 0.00021779456319753898, Validation Loss 0.5332509792924423\n",
            "Epoch 59: Validation Accuracy: 0.95, Training Loss: 0.00021371266869510384, Validation Loss 0.5313927069794191\n",
            "Epoch 60: Validation Accuracy: 0.95, Training Loss: 0.0001954923531150549, Validation Loss 0.5414358330391666\n",
            "Epoch 61: Validation Accuracy: 0.95, Training Loss: 0.00018683358488961717, Validation Loss 0.5476246523203927\n",
            "Epoch 62: Validation Accuracy: 0.95, Training Loss: 0.00019986193070553604, Validation Loss 0.5595992009596233\n",
            "Epoch 63: Validation Accuracy: 0.95, Training Loss: 0.00019316917899502814, Validation Loss 0.5618255131323022\n",
            "Epoch 64: Validation Accuracy: 0.95, Training Loss: 0.00018675741999029557, Validation Loss 0.5664048970184601\n",
            "Epoch 65: Validation Accuracy: 0.95, Training Loss: 0.000179149135006666, Validation Loss 0.5826078868269772\n",
            "Epoch 66: Validation Accuracy: 0.95, Training Loss: 0.0002035655148509897, Validation Loss 0.585368291356168\n",
            "Epoch 67: Validation Accuracy: 0.93, Training Loss: 0.018684009536670404, Validation Loss 0.5237682735960861\n",
            "Epoch 68: Validation Accuracy: 0.95, Training Loss: 0.07563508679324243, Validation Loss 0.2535787405493616\n",
            "Epoch 69: Validation Accuracy: 0.95, Training Loss: 0.013209526472371504, Validation Loss 0.326705670925353\n",
            "Epoch 70: Validation Accuracy: 0.96, Training Loss: 0.0026230133249844807, Validation Loss 0.348411073906718\n",
            "Epoch 71: Validation Accuracy: 0.96, Training Loss: 0.000731261901007142, Validation Loss 0.37023169531678446\n",
            "Epoch 72: Validation Accuracy: 0.95, Training Loss: 0.0003909327325377828, Validation Loss 0.3978480362023987\n",
            "Epoch 73: Validation Accuracy: 0.95, Training Loss: 0.00029502354636195513, Validation Loss 0.3965187116991874\n",
            "Epoch 74: Validation Accuracy: 0.95, Training Loss: 0.00025222494435731815, Validation Loss 0.41041824891492007\n",
            "Epoch 75: Validation Accuracy: 0.95, Training Loss: 0.00022297659767610186, Validation Loss 0.4176789838675353\n",
            "Epoch 76: Validation Accuracy: 0.95, Training Loss: 0.00020380490663791842, Validation Loss 0.4247349495728519\n",
            "Epoch 77: Validation Accuracy: 0.95, Training Loss: 0.0001918839969200048, Validation Loss 0.43818857588461513\n",
            "Epoch 78: Validation Accuracy: 0.95, Training Loss: 0.0001882258598110731, Validation Loss 0.44547958406972965\n",
            "Epoch 79: Validation Accuracy: 0.95, Training Loss: 0.00017816944738782256, Validation Loss 0.4484842891541899\n",
            "Epoch 80: Validation Accuracy: 0.95, Training Loss: 0.00016291717104358012, Validation Loss 0.4623826057702799\n",
            "Epoch 81: Validation Accuracy: 0.95, Training Loss: 0.0001587727329850417, Validation Loss 0.4694620399561078\n",
            "Epoch 82: Validation Accuracy: 0.95, Training Loss: 0.00015742638974125346, Validation Loss 0.47228625877282526\n",
            "Epoch 83: Validation Accuracy: 0.95, Training Loss: 0.00015141778942667067, Validation Loss 0.47678814949548864\n",
            "Epoch 84: Validation Accuracy: 0.95, Training Loss: 0.00014338653563463814, Validation Loss 0.48662309173084656\n",
            "Epoch 85: Validation Accuracy: 0.95, Training Loss: 0.0001408405341522279, Validation Loss 0.49513670613110494\n",
            "Epoch 86: Validation Accuracy: 0.95, Training Loss: 0.0001360434955054884, Validation Loss 0.4962826483908791\n",
            "Epoch 87: Validation Accuracy: 0.95, Training Loss: 0.000132179220203337, Validation Loss 0.5050998151878523\n",
            "Epoch 88: Validation Accuracy: 0.95, Training Loss: 0.0001326006636617357, Validation Loss 0.5076115270418141\n",
            "Epoch 89: Validation Accuracy: 0.95, Training Loss: 0.00012675692734562404, Validation Loss 0.5283239562684956\n",
            "Epoch 90: Validation Accuracy: 0.95, Training Loss: 0.00012544114248471446, Validation Loss 0.5215202717873302\n",
            "Epoch 91: Validation Accuracy: 0.95, Training Loss: 0.00012080227037484668, Validation Loss 0.5282464038066481\n",
            "Epoch 92: Validation Accuracy: 0.95, Training Loss: 0.00012103872165260062, Validation Loss 0.5332699221224148\n",
            "Epoch 93: Validation Accuracy: 0.95, Training Loss: 0.00011181093802729496, Validation Loss 0.539314921213299\n",
            "Epoch 94: Validation Accuracy: 0.94, Training Loss: 0.00010801770142685327, Validation Loss 0.5449854606600348\n",
            "Epoch 95: Validation Accuracy: 0.95, Training Loss: 0.00010821105239941834, Validation Loss 0.5431486336234532\n",
            "Epoch 96: Validation Accuracy: 0.95, Training Loss: 0.00011112429575373783, Validation Loss 0.5528082928304273\n",
            "Epoch 97: Validation Accuracy: 0.95, Training Loss: 0.00010635955163130265, Validation Loss 0.5576060182199103\n",
            "Epoch 98: Validation Accuracy: 0.94, Training Loss: 0.00010129702674261321, Validation Loss 0.5608711793007581\n",
            "Epoch 99: Validation Accuracy: 0.94, Training Loss: 0.0001023743357796844, Validation Loss 0.5628743981901672\n",
            "Epoch 100: Validation Accuracy: 0.94, Training Loss: 9.536759470084353e-05, Validation Loss 0.5683565106360079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.MLPClassifier at 0x7f45de0c64d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDaMbl9jhkOU"
      },
      "source": [
        "import dill\n",
        "afn_name = \"xavier_dropout_0_2\"\n",
        "opt_name = \"gdm_relu_xavier_dropout_0_2\"\n",
        "with open('MLP_{}_{}.pkl'.format(opt_name, afn_name), 'wb') as f:\n",
        "    dill.dump(mlp, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "NuCF263HROYT",
        "outputId": "6a0b79d8-4e90-496f-d647-87e1649c5d55"
      },
      "source": [
        "mlp.plot_roc(X_test, y_test, \"{}_{}_ROC\".format(opt_name, afn_name))\n",
        "mlp.plot_losses(hidden_layer_afn=\"{}\".format(opt_name, afn_name), optimizer_name=\"{}\".format(opt_name), plot_name=\"{}_{}_Losses\".format(opt_name, afn_name))\n",
        "mlp.plot_confusion_matrix(X_test, y_test, \"{}_{}_CM\".format(opt_name, afn_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[194   0   1   0   1   0   2   0   1   1]\n",
            " [  0 196   1   1   0   0   0   0   2   0]\n",
            " [  1   0 186   1   0   0   5   3   3   1]\n",
            " [  0   0   4 187   0   2   0   5   1   1]\n",
            " [  1   0   1   0 186   0   5   0   0   7]\n",
            " [  1   0   0   6   4 177   3   1   7   1]\n",
            " [  3   2   3   0   6   3 181   0   1   1]\n",
            " [  0   3   5   2   1   0   0 183   0   6]\n",
            " [  1   2   5   5   2   1   1   2 177   4]\n",
            " [  1   0   0   1   3   1   0   4   6 184]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sY6Tki4obCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510027ab-28cc-4ff8-ed38-6727f4b375b1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[187   0   2   3   1   5   2   0   0   0]\n",
            " [  0 192   1   2   0   1   1   0   3   0]\n",
            " [  1   2 160   9   0   1   6   5  13   3]\n",
            " [  0   0   1 169   0  10   2   6  10   2]\n",
            " [  0   1   1   0 153   0   7   2   1  35]\n",
            " [  4   0   0  29   3 118   7  11  17  11]\n",
            " [  8   2   9   1   9   7 158   0   6   0]\n",
            " [  1  10   8   1   2   0   0 160   4  14]\n",
            " [  3   1   8  14   3   4   2   3 149  13]\n",
            " [  1   1   0  10   5   0   0   3   4 176]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOEfapC-obFA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekFFemfp_CGy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7md6nEIx1eM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}